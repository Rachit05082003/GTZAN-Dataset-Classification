{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# CNN + Attention: Spectrogram Images + CSV Features\n",
                "\n",
                "**Highest Accuracy Approach**\n",
                "\n",
                "This notebook combines TWO data sources for best performance:\n",
                "1. **PNG Spectrograms** → CNN + CBAM Attention\n",
                "2. **CSV Features (57 audio features)** → Dense Network\n",
                "3. **Fusion** → Multi-Head Attention → Classification\n",
                "\n",
                "**Expected Performance:** 90-95% accuracy"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Imports"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
                "\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "from PIL import Image\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "import tensorflow as tf\n",
                "from tensorflow.keras import layers, Model, regularizers\n",
                "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
                "from tensorflow.keras.utils import to_categorical\n",
                "\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
                "from sklearn.metrics import classification_report, confusion_matrix\n",
                "\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from tqdm import tqdm\n",
                "\n",
                "np.random.seed(42)\n",
                "tf.random.set_seed(42)\n",
                "\n",
                "plt.style.use('seaborn-v0_8-darkgrid')\n",
                "\n",
                "print(f\"TensorFlow: {tf.__version__}\")\n",
                "print(f\"GPU: {len(tf.config.list_physical_devices('GPU')) > 0}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ==================== PATHS ====================\n",
                "BASE_PATH = '/Users/narac0503/GIT/GTZAN Dataset Classification/GTZAN-Dataset-Classification/gtzan-classification/data/gtzan'\n",
                "IMAGE_PATH = os.path.join(BASE_PATH, 'images_original')\n",
                "CSV_PATH = os.path.join(BASE_PATH, 'features_30_sec.csv')\n",
                "\n",
                "print(f\"Images exist: {os.path.exists(IMAGE_PATH)}\")\n",
                "print(f\"CSV exists: {os.path.exists(CSV_PATH)}\")\n",
                "\n",
                "# ==================== IMAGE SETTINGS ====================\n",
                "TARGET_SIZE = (224, 224)  # Larger size for more detail\n",
                "\n",
                "# ==================== MODEL ====================\n",
                "NUM_CLASSES = 10\n",
                "GENRES = ['blues', 'classical', 'country', 'disco', 'hiphop',\n",
                "          'jazz', 'metal', 'pop', 'reggae', 'rock']\n",
                "\n",
                "# ==================== HYPERPARAMETERS ====================\n",
                "CNN_FILTERS = [64, 128, 256, 512]\n",
                "ATTENTION_HEADS = 8\n",
                "DENSE_UNITS = 512\n",
                "DROPOUT_RATE = 0.5\n",
                "LEARNING_RATE = 0.001\n",
                "BATCH_SIZE = 32\n",
                "EPOCHS = 150"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. CBAM Attention Layer"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class CBAM(layers.Layer):\n",
                "    \"\"\"Convolutional Block Attention Module.\"\"\"\n",
                "    \n",
                "    def __init__(self, reduction=16, **kwargs):\n",
                "        super().__init__(**kwargs)\n",
                "        self.reduction = reduction\n",
                "    \n",
                "    def build(self, input_shape):\n",
                "        channels = input_shape[-1]\n",
                "        self.fc1 = layers.Dense(max(channels // self.reduction, 1), activation='relu')\n",
                "        self.fc2 = layers.Dense(channels)\n",
                "        self.conv_spatial = layers.Conv2D(1, 7, padding='same')\n",
                "    \n",
                "    def call(self, x):\n",
                "        # Channel attention\n",
                "        avg_pool = tf.reduce_mean(x, axis=[1, 2])\n",
                "        max_pool = tf.reduce_max(x, axis=[1, 2])\n",
                "        \n",
                "        avg_out = self.fc2(self.fc1(avg_pool))\n",
                "        max_out = self.fc2(self.fc1(max_pool))\n",
                "        \n",
                "        channel_attn = tf.nn.sigmoid(avg_out + max_out)\n",
                "        channel_attn = tf.reshape(channel_attn, [-1, 1, 1, tf.shape(x)[-1]])\n",
                "        x = x * channel_attn\n",
                "        \n",
                "        # Spatial attention\n",
                "        avg_spatial = tf.reduce_mean(x, axis=-1, keepdims=True)\n",
                "        max_spatial = tf.reduce_max(x, axis=-1, keepdims=True)\n",
                "        concat = tf.concat([avg_spatial, max_spatial], axis=-1)\n",
                "        spatial_attn = tf.nn.sigmoid(self.conv_spatial(concat))\n",
                "        \n",
                "        return x * spatial_attn\n",
                "\n",
                "print(\"CBAM defined.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Load Images"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def load_image(path, target_size=TARGET_SIZE):\n",
                "    try:\n",
                "        img = Image.open(path)\n",
                "        if img.mode != 'RGB':\n",
                "            img = img.convert('RGB')\n",
                "        img = img.resize(target_size)\n",
                "        return np.array(img) / 255.0\n",
                "    except:\n",
                "        return None\n",
                "\n",
                "def load_images(image_path):\n",
                "    X_img, filenames, labels = [], [], []\n",
                "    \n",
                "    print(\"Loading images...\\n\")\n",
                "    for genre in GENRES:\n",
                "        genre_path = os.path.join(image_path, genre)\n",
                "        if not os.path.exists(genre_path):\n",
                "            continue\n",
                "        \n",
                "        files = sorted([f for f in os.listdir(genre_path) if f.endswith('.png')])\n",
                "        print(f\"{genre}: {len(files)} images\")\n",
                "        \n",
                "        for f in tqdm(files, desc=genre):\n",
                "            img = load_image(os.path.join(genre_path, f))\n",
                "            if img is not None:\n",
                "                X_img.append(img)\n",
                "                # Create matching filename for CSV lookup\n",
                "                # e.g., \"pop00005.png\" → \"pop.00005.wav\"\n",
                "                base = f.replace('.png', '')\n",
                "                wav_name = f\"{genre}.{base[len(genre):].zfill(5)}.wav\"\n",
                "                filenames.append(wav_name)\n",
                "                labels.append(genre)\n",
                "    \n",
                "    return np.array(X_img), filenames, labels\n",
                "\n",
                "X_images, filenames, labels = load_images(IMAGE_PATH)\n",
                "print(f\"\\nLoaded {len(X_images)} images\")\n",
                "print(f\"Shape: {X_images.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Load CSV Features"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load CSV\n",
                "df = pd.read_csv(CSV_PATH)\n",
                "print(f\"CSV shape: {df.shape}\")\n",
                "print(f\"Columns: {list(df.columns[:10])}...\")\n",
                "\n",
                "# Get feature columns (drop filename, length, label)\n",
                "feature_cols = [c for c in df.columns if c not in ['filename', 'length', 'label']]\n",
                "print(f\"\\nNumber of features: {len(feature_cols)}\")\n",
                "\n",
                "# Match CSV features to images by filename\n",
                "X_csv = []\n",
                "matched_indices = []\n",
                "\n",
                "for i, fname in enumerate(filenames):\n",
                "    # Try different filename formats\n",
                "    row = df[df['filename'] == fname]\n",
                "    if len(row) == 0:\n",
                "        # Try without leading zeros\n",
                "        parts = fname.split('.')\n",
                "        if len(parts) >= 2:\n",
                "            alt_fname = f\"{parts[0]}.{int(parts[1]):05d}.wav\"\n",
                "            row = df[df['filename'] == alt_fname]\n",
                "    \n",
                "    if len(row) > 0:\n",
                "        X_csv.append(row[feature_cols].values[0])\n",
                "        matched_indices.append(i)\n",
                "\n",
                "X_csv = np.array(X_csv)\n",
                "print(f\"\\nMatched {len(X_csv)} samples with CSV features\")\n",
                "\n",
                "# Filter images and labels to matched only\n",
                "X_images = X_images[matched_indices]\n",
                "labels = [labels[i] for i in matched_indices]\n",
                "\n",
                "print(f\"Final images: {X_images.shape}\")\n",
                "print(f\"Final CSV: {X_csv.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Preprocessing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Encode labels\n",
                "le = LabelEncoder()\n",
                "y_encoded = le.fit_transform(labels)\n",
                "y_onehot = to_categorical(y_encoded, NUM_CLASSES)\n",
                "\n",
                "print(f\"Labels: {y_onehot.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Train/Val/Test Split"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Indices for splitting\n",
                "indices = np.arange(len(X_images))\n",
                "\n",
                "# Split\n",
                "idx_temp, idx_test = train_test_split(indices, test_size=0.1, stratify=y_encoded, random_state=42)\n",
                "idx_train, idx_val = train_test_split(idx_temp, test_size=0.111, stratify=y_encoded[idx_temp], random_state=42)\n",
                "\n",
                "# Images\n",
                "X_img_train, X_img_val, X_img_test = X_images[idx_train], X_images[idx_val], X_images[idx_test]\n",
                "\n",
                "# CSV features\n",
                "X_csv_train, X_csv_val, X_csv_test = X_csv[idx_train], X_csv[idx_val], X_csv[idx_test]\n",
                "\n",
                "# Labels\n",
                "y_train, y_val, y_test = y_onehot[idx_train], y_onehot[idx_val], y_onehot[idx_test]\n",
                "\n",
                "print(f\"Train: {len(idx_train)}\")\n",
                "print(f\"Val: {len(idx_val)}\")\n",
                "print(f\"Test: {len(idx_test)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Normalize CSV Features"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Normalize CSV features\n",
                "scaler = StandardScaler()\n",
                "X_csv_train = scaler.fit_transform(X_csv_train)\n",
                "X_csv_val = scaler.transform(X_csv_val)\n",
                "X_csv_test = scaler.transform(X_csv_test)\n",
                "\n",
                "print(f\"CSV normalized - Mean: {X_csv_train.mean():.4f}, Std: {X_csv_train.std():.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Build Dual-Input Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def build_dual_input_model(image_shape, csv_dim):\n",
                "    \"\"\"\n",
                "    Dual-input model:\n",
                "    1. CNN + CBAM for images\n",
                "    2. Dense network for CSV features\n",
                "    3. Multi-head attention for fusion\n",
                "    \"\"\"\n",
                "    \n",
                "    # ==================== IMAGE BRANCH ====================\n",
                "    img_input = layers.Input(shape=image_shape, name='image_input')\n",
                "    x = img_input\n",
                "    \n",
                "    for filters in CNN_FILTERS:\n",
                "        x = layers.Conv2D(filters, 3, padding='same')(x)\n",
                "        x = layers.BatchNormalization()(x)\n",
                "        x = layers.Activation('relu')(x)\n",
                "        x = CBAM()(x)\n",
                "        x = layers.MaxPooling2D(2)(x)\n",
                "        x = layers.Dropout(0.25)(x)\n",
                "    \n",
                "    x = layers.GlobalAveragePooling2D()(x)\n",
                "    img_features = layers.Dense(256, activation='relu')(x)\n",
                "    img_features = layers.Dropout(0.3)(img_features)\n",
                "    \n",
                "    # ==================== CSV BRANCH ====================\n",
                "    csv_input = layers.Input(shape=(csv_dim,), name='csv_input')\n",
                "    \n",
                "    y = layers.Dense(256, activation='relu')(csv_input)\n",
                "    y = layers.BatchNormalization()(y)\n",
                "    y = layers.Dropout(0.3)(y)\n",
                "    y = layers.Dense(128, activation='relu')(y)\n",
                "    y = layers.BatchNormalization()(y)\n",
                "    csv_features = layers.Dropout(0.3)(y)\n",
                "    \n",
                "    # ==================== FUSION WITH ATTENTION ====================\n",
                "    # Stack as sequence for attention\n",
                "    img_expanded = layers.Reshape((1, 256))(img_features)\n",
                "    csv_expanded = layers.Reshape((1, 128))(csv_features)\n",
                "    \n",
                "    # Pad CSV to match dimensions\n",
                "    csv_padded = layers.Dense(256)(layers.Reshape((128,))(csv_expanded))\n",
                "    csv_padded = layers.Reshape((1, 256))(csv_padded)\n",
                "    \n",
                "    # Concatenate as sequence\n",
                "    combined = layers.Concatenate(axis=1)([img_expanded, csv_padded])\n",
                "    \n",
                "    # Multi-head attention\n",
                "    attn = layers.MultiHeadAttention(\n",
                "        num_heads=ATTENTION_HEADS,\n",
                "        key_dim=32\n",
                "    )(combined, combined)\n",
                "    \n",
                "    attended = layers.GlobalAveragePooling1D()(attn)\n",
                "    \n",
                "    # ==================== CLASSIFICATION ====================\n",
                "    z = layers.Dense(DENSE_UNITS, activation='relu')(attended)\n",
                "    z = layers.BatchNormalization()(z)\n",
                "    z = layers.Dropout(DROPOUT_RATE)(z)\n",
                "    \n",
                "    z = layers.Dense(256, activation='relu')(z)\n",
                "    z = layers.Dropout(0.3)(z)\n",
                "    \n",
                "    outputs = layers.Dense(NUM_CLASSES, activation='softmax')(z)\n",
                "    \n",
                "    # ==================== COMPILE ====================\n",
                "    model = Model(inputs=[img_input, csv_input], outputs=outputs)\n",
                "    \n",
                "    model.compile(\n",
                "        optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
                "        loss='categorical_crossentropy',\n",
                "        metrics=['accuracy']\n",
                "    )\n",
                "    \n",
                "    return model\n",
                "\n",
                "\n",
                "model = build_dual_input_model(X_img_train.shape[1:], X_csv_train.shape[1])\n",
                "model.summary()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. Training"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "callbacks = [\n",
                "    EarlyStopping(monitor='val_accuracy', patience=25, restore_best_weights=True, verbose=1),\n",
                "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=8, min_lr=1e-7, verbose=1),\n",
                "    ModelCheckpoint('best_dual_input.keras', monitor='val_accuracy', save_best_only=True, verbose=1)\n",
                "]\n",
                "\n",
                "print(\"\\nTraining dual-input model...\\n\")\n",
                "history = model.fit(\n",
                "    [X_img_train, X_csv_train], y_train,\n",
                "    validation_data=([X_img_val, X_csv_val], y_val),\n",
                "    batch_size=BATCH_SIZE,\n",
                "    epochs=EPOCHS,\n",
                "    callbacks=callbacks,\n",
                "    verbose=1\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 11. Training History"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "ax1.plot(history.history['accuracy'], label='Train')\n",
                "ax1.plot(history.history['val_accuracy'], label='Val')\n",
                "ax1.set_title('Accuracy', fontweight='bold')\n",
                "ax1.legend()\n",
                "ax1.grid(alpha=0.3)\n",
                "\n",
                "ax2.plot(history.history['loss'], label='Train')\n",
                "ax2.plot(history.history['val_loss'], label='Val')\n",
                "ax2.set_title('Loss', fontweight='bold')\n",
                "ax2.legend()\n",
                "ax2.grid(alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('dual_input_history.png', dpi=300)\n",
                "plt.show()\n",
                "\n",
                "print(f\"\\nBest Val Accuracy: {max(history.history['val_accuracy']):.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 12. Evaluation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model.load_weights('best_dual_input.keras')\n",
                "\n",
                "test_loss, test_acc = model.evaluate([X_img_test, X_csv_test], y_test, verbose=0)\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(f\"TEST ACCURACY: {test_acc:.4f} ({test_acc*100:.2f}%)\")\n",
                "print(f\"TEST LOSS: {test_loss:.4f}\")\n",
                "print(\"=\"*60)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 13. Classification Report & Confusion Matrix"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "y_pred = model.predict([X_img_test, X_csv_test], verbose=0)\n",
                "y_pred_labels = np.argmax(y_pred, axis=1)\n",
                "y_true_labels = np.argmax(y_test, axis=1)\n",
                "\n",
                "print(\"\\nClassification Report:\")\n",
                "print(\"=\"*60)\n",
                "print(classification_report(y_true_labels, y_pred_labels, target_names=GENRES, digits=3))\n",
                "\n",
                "cm = confusion_matrix(y_true_labels, y_pred_labels)\n",
                "\n",
                "plt.figure(figsize=(12, 10))\n",
                "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
                "            xticklabels=GENRES, yticklabels=GENRES)\n",
                "plt.xlabel('Predicted', fontweight='bold')\n",
                "plt.ylabel('True', fontweight='bold')\n",
                "plt.title(f'Dual-Input CNN+Attention (Acc: {test_acc:.2%})', fontweight='bold')\n",
                "plt.xticks(rotation=45, ha='right')\n",
                "plt.tight_layout()\n",
                "plt.savefig('dual_input_cm.png', dpi=300)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 14. Save"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model.save('dual_input_final.keras')\n",
                "np.save('dual_input_history.npy', history.history)\n",
                "\n",
                "import joblib\n",
                "joblib.dump(scaler, 'dual_input_scaler.pkl')\n",
                "\n",
                "print(\"Saved:\")\n",
                "print(\"  ✓ dual_input_final.keras\")\n",
                "print(\"  ✓ best_dual_input.keras\")\n",
                "print(\"  ✓ dual_input_scaler.pkl\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Summary\n",
                "\n",
                "**Dual-Input Architecture:**\n",
                "```\n",
                "PNG Spectrogram → CNN + CBAM → Image Features (256-dim)\n",
                "                                                        ↘\n",
                "                                                         → Attention Fusion → Classifier\n",
                "                                                        ↗\n",
                "CSV Features → Dense Network → Audio Features (128-dim)\n",
                "```\n",
                "\n",
                "**Why This Works Better:**\n",
                "- Images capture **visual patterns** (texture, structure)\n",
                "- CSV features capture **statistical audio properties** (MFCC means, spectral features)\n",
                "- Attention learns **which modality is more important** for each genre\n",
                "- Fusion combines complementary information\n",
                "\n",
                "**Expected:** 90-95% accuracy (much better than single-input!)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "gtzan",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.9.25"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}