{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Method 1: LSTM with Pre-extracted CSV Features\n",
                "\n",
                "**Key Improvements:**\n",
                "- Uses pre-extracted features from CSV files (no need to extract MFCC)\n",
                "- Trains on 3-second segments (features_3_sec.csv)\n",
                "- Tests on 30-second aggregated features (features_30_sec.csv)\n",
                "- Faster training and more features available\n",
                "- Bidirectional LSTM for better temporal modeling"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Imports"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "import tensorflow as tf\n",
                "from tensorflow.keras.models import Sequential\n",
                "from tensorflow.keras.layers import LSTM, Bidirectional, Dense, Dropout, BatchNormalization\n",
                "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
                "from tensorflow.keras.utils import to_categorical\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
                "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# Plot styling\n",
                "plt.style.use('seaborn-v0_8-darkgrid')\n",
                "sns.set_palette('husl')\n",
                "\n",
                "# Reproducibility\n",
                "np.random.seed(42)\n",
                "tf.random.set_seed(42)\n",
                "\n",
                "print(f'TensorFlow version: {tf.__version__}')\n",
                "print(f'GPU Available: {len(tf.config.list_physical_devices(\"GPU\")) > 0}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Load CSV Features"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load datasets\n",
                "df_3sec = pd.read_csv('../data/gtzan/features_3_sec.csv')\n",
                "df_30sec = pd.read_csv('../data/gtzan/features_30_sec.csv')\n",
                "\n",
                "print(f\"3-second features shape: {df_3sec.shape}\")\n",
                "print(f\"30-second features shape: {df_30sec.shape}\")\n",
                "print(f\"\\nColumn names (first 10): {list(df_3sec.columns[:10])}\")\n",
                "print(f\"\\nGenres in dataset: {df_3sec['label'].unique()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Data Preprocessing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Drop unnecessary columns\n",
                "columns_to_drop = ['filename', 'length']\n",
                "df_3sec_clean = df_3sec.drop(columns=[col for col in columns_to_drop if col in df_3sec.columns])\n",
                "df_30sec_clean = df_30sec.drop(columns=[col for col in columns_to_drop if col in df_30sec.columns])\n",
                "\n",
                "# Separate features and labels for 3-second data (training)\n",
                "X_train_val = df_3sec_clean.drop('label', axis=1).values\n",
                "y_train_val = df_3sec_clean['label'].values\n",
                "\n",
                "# Separate features and labels for 30-second data (testing)\n",
                "X_test = df_30sec_clean.drop('label', axis=1).values\n",
                "y_test = df_30sec_clean['label'].values\n",
                "\n",
                "print(f\"Training features shape: {X_train_val.shape}\")\n",
                "print(f\"Training labels shape: {y_train_val.shape}\")\n",
                "print(f\"Test features shape: {X_test.shape}\")\n",
                "print(f\"Test labels shape: {y_test.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Encode Labels"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Encode labels\n",
                "label_encoder = LabelEncoder()\n",
                "y_train_val_encoded = label_encoder.fit_transform(y_train_val)\n",
                "y_test_encoded = label_encoder.transform(y_test)\n",
                "\n",
                "# Get genre names\n",
                "genre_names = label_encoder.classes_\n",
                "num_classes = len(genre_names)\n",
                "\n",
                "print(f\"Number of classes: {num_classes}\")\n",
                "print(f\"Genre names: {genre_names}\")\n",
                "\n",
                "# One-hot encode\n",
                "y_train_val_cat = to_categorical(y_train_val_encoded, num_classes)\n",
                "y_test_cat = to_categorical(y_test_encoded, num_classes)\n",
                "\n",
                "print(f\"\\nOne-hot encoded shape: {y_train_val_cat.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Train/Validation Split"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Split training data into train and validation\n",
                "X_train, X_val, y_train, y_val = train_test_split(\n",
                "    X_train_val, y_train_val_cat,\n",
                "    test_size=0.2,\n",
                "    random_state=42,\n",
                "    stratify=y_train_val_encoded\n",
                ")\n",
                "\n",
                "print(f\"Train set: {X_train.shape}\")\n",
                "print(f\"Validation set: {X_val.shape}\")\n",
                "print(f\"Test set: {X_test.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Feature Scaling"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Standardize features\n",
                "scaler = StandardScaler()\n",
                "X_train_scaled = scaler.fit_transform(X_train)\n",
                "X_val_scaled = scaler.transform(X_val)\n",
                "X_test_scaled = scaler.transform(X_test)\n",
                "\n",
                "# Reshape for LSTM (samples, timesteps, features)\n",
                "# For LSTM, we'll treat each feature as a timestep\n",
                "X_train_lstm = X_train_scaled.reshape(X_train_scaled.shape[0], X_train_scaled.shape[1], 1)\n",
                "X_val_lstm = X_val_scaled.reshape(X_val_scaled.shape[0], X_val_scaled.shape[1], 1)\n",
                "X_test_lstm = X_test_scaled.reshape(X_test_scaled.shape[0], X_test_scaled.shape[1], 1)\n",
                "\n",
                "print(f\"\\nReshaped for LSTM:\")\n",
                "print(f\"Train shape: {X_train_lstm.shape}\")\n",
                "print(f\"Val shape: {X_val_lstm.shape}\")\n",
                "print(f\"Test shape: {X_test_lstm.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Build Bidirectional LSTM Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def build_lstm_model(input_shape, num_classes):\n",
                "    \"\"\"\n",
                "    Build improved Bidirectional LSTM model.\n",
                "    \n",
                "    Architecture:\n",
                "    - Bidirectional LSTM layers for better temporal modeling\n",
                "    - Batch normalization for stable training\n",
                "    - Dropout for regularization\n",
                "    - Dense layers for classification\n",
                "    \"\"\"\n",
                "    model = Sequential([\n",
                "        # First Bidirectional LSTM layer\n",
                "        Bidirectional(LSTM(128, return_sequences=True), input_shape=input_shape),\n",
                "        BatchNormalization(),\n",
                "        Dropout(0.3),\n",
                "        \n",
                "        # Second Bidirectional LSTM layer\n",
                "        Bidirectional(LSTM(64, return_sequences=True)),\n",
                "        BatchNormalization(),\n",
                "        Dropout(0.3),\n",
                "        \n",
                "        # Third LSTM layer (not bidirectional to reduce parameters)\n",
                "        LSTM(32),\n",
                "        BatchNormalization(),\n",
                "        Dropout(0.4),\n",
                "        \n",
                "        # Dense layers\n",
                "        Dense(64, activation='relu'),\n",
                "        Dropout(0.4),\n",
                "        Dense(32, activation='relu'),\n",
                "        Dropout(0.3),\n",
                "        \n",
                "        # Output layer\n",
                "        Dense(num_classes, activation='softmax')\n",
                "    ], name='bidirectional_lstm')\n",
                "    \n",
                "    # Compile\n",
                "    model.compile(\n",
                "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
                "        loss='categorical_crossentropy',\n",
                "        metrics=['accuracy']\n",
                "    )\n",
                "    \n",
                "    return model\n",
                "\n",
                "# Build model\n",
                "model = build_lstm_model(\n",
                "    input_shape=(X_train_lstm.shape[1], X_train_lstm.shape[2]),\n",
                "    num_classes=num_classes\n",
                ")\n",
                "\n",
                "# Display model architecture\n",
                "model.summary()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Training"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Callbacks\n",
                "early_stop = EarlyStopping(\n",
                "    monitor='val_loss',\n",
                "    patience=20,\n",
                "    restore_best_weights=True,\n",
                "    verbose=1\n",
                ")\n",
                "\n",
                "reduce_lr = ReduceLROnPlateau(\n",
                "    monitor='val_loss',\n",
                "    factor=0.5,\n",
                "    patience=5,\n",
                "    min_lr=1e-6,\n",
                "    verbose=1\n",
                ")\n",
                "\n",
                "# Train model\n",
                "history = model.fit(\n",
                "    X_train_lstm, y_train,\n",
                "    validation_data=(X_val_lstm, y_val),\n",
                "    batch_size=64,\n",
                "    epochs=100,\n",
                "    callbacks=[early_stop, reduce_lr],\n",
                "    verbose=1\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Training History"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot training history\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "# Loss\n",
                "axes[0].plot(history.history['loss'], label='Train Loss', linewidth=2)\n",
                "axes[0].plot(history.history['val_loss'], label='Val Loss', linewidth=2)\n",
                "axes[0].set_xlabel('Epoch', fontsize=12)\n",
                "axes[0].set_ylabel('Loss', fontsize=12)\n",
                "axes[0].set_title('Training & Validation Loss', fontsize=14, fontweight='bold')\n",
                "axes[0].legend()\n",
                "axes[0].grid(True, alpha=0.3)\n",
                "\n",
                "# Accuracy\n",
                "axes[1].plot(history.history['accuracy'], label='Train Accuracy', linewidth=2)\n",
                "axes[1].plot(history.history['val_accuracy'], label='Val Accuracy', linewidth=2)\n",
                "axes[1].set_xlabel('Epoch', fontsize=12)\n",
                "axes[1].set_ylabel('Accuracy', fontsize=12)\n",
                "axes[1].set_title('Training & Validation Accuracy', fontsize=14, fontweight='bold')\n",
                "axes[1].legend()\n",
                "axes[1].grid(True, alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('lstm_csv_training_history.png', dpi=300, bbox_inches='tight')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. Evaluation on Test Set"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Evaluate on test set (30-second features)\n",
                "test_loss, test_acc = model.evaluate(X_test_lstm, y_test_cat, verbose=0)\n",
                "\n",
                "print(f\"\\n{'='*60}\")\n",
                "print(f\"Test Accuracy (30-second features): {test_acc*100:.2f}%\")\n",
                "print(f\"Test Loss: {test_loss:.4f}\")\n",
                "print(f\"{'='*60}\\n\")\n",
                "\n",
                "# Predictions\n",
                "y_pred = model.predict(X_test_lstm, verbose=0)\n",
                "y_pred_labels = np.argmax(y_pred, axis=1)\n",
                "y_true_labels = np.argmax(y_test_cat, axis=1)\n",
                "\n",
                "# Classification report\n",
                "print(\"\\nClassification Report:\")\n",
                "print(classification_report(y_true_labels, y_pred_labels, target_names=genre_names))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 11. Confusion Matrix"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Confusion matrix\n",
                "cm = confusion_matrix(y_true_labels, y_pred_labels)\n",
                "\n",
                "plt.figure(figsize=(12, 10))\n",
                "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
                "            xticklabels=genre_names, yticklabels=genre_names,\n",
                "            cbar_kws={'label': 'Count'})\n",
                "plt.xlabel('Predicted Genre', fontsize=12, fontweight='bold')\n",
                "plt.ylabel('True Genre', fontsize=12, fontweight='bold')\n",
                "plt.title('Bidirectional LSTM - Confusion Matrix', fontsize=14, fontweight='bold')\n",
                "plt.xticks(rotation=45, ha='right')\n",
                "plt.yticks(rotation=0)\n",
                "plt.tight_layout()\n",
                "plt.savefig('lstm_csv_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 12. Per-Genre Accuracy"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Calculate per-genre accuracy\n",
                "per_genre_acc = []\n",
                "for i, genre in enumerate(genre_names):\n",
                "    genre_mask = y_true_labels == i\n",
                "    genre_acc = accuracy_score(\n",
                "        y_true_labels[genre_mask],\n",
                "        y_pred_labels[genre_mask]\n",
                "    )\n",
                "    per_genre_acc.append(genre_acc * 100)\n",
                "\n",
                "# Plot per-genre accuracy\n",
                "plt.figure(figsize=(12, 6))\n",
                "bars = plt.bar(genre_names, per_genre_acc, color='steelblue', edgecolor='black')\n",
                "plt.axhline(y=test_acc*100, color='red', linestyle='--', \n",
                "            label=f'Overall Accuracy: {test_acc*100:.2f}%', linewidth=2)\n",
                "plt.xlabel('Genre', fontsize=12, fontweight='bold')\n",
                "plt.ylabel('Accuracy (%)', fontsize=12, fontweight='bold')\n",
                "plt.title('Per-Genre Classification Accuracy', fontsize=14, fontweight='bold')\n",
                "plt.xticks(rotation=45, ha='right')\n",
                "plt.ylim([0, 100])\n",
                "plt.grid(True, alpha=0.3, axis='y')\n",
                "plt.legend()\n",
                "\n",
                "# Add value labels on bars\n",
                "for bar, acc in zip(bars, per_genre_acc):\n",
                "    height = bar.get_height()\n",
                "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
                "             f'{acc:.1f}%', ha='center', va='bottom', fontsize=10)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('lstm_csv_per_genre_accuracy.png', dpi=300, bbox_inches='tight')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 13. Save Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save model\n",
                "model.save('lstm_csv_features.keras')\n",
                "print(\"✓ Model saved: lstm_csv_features.keras\")\n",
                "\n",
                "# Save training history\n",
                "np.save('lstm_csv_history.npy', history.history)\n",
                "print(\"✓ Training history saved: lstm_csv_history.npy\")\n",
                "\n",
                "# Save scaler and label encoder\n",
                "import joblib\n",
                "joblib.dump(scaler, 'lstm_scaler.pkl')\n",
                "joblib.dump(label_encoder, 'lstm_label_encoder.pkl')\n",
                "print(\"✓ Scaler and label encoder saved\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Summary\n",
                "\n",
                "This improved LSTM model uses pre-extracted CSV features:\n",
                "\n",
                "**Data Strategy:**\n",
                "- Training: 3-second segment features (more data samples)\n",
                "- Testing: 30-second aggregated features (realistic full-track evaluation)\n",
                "- No need to extract audio features (much faster)\n",
                "\n",
                "**Model Architecture:**\n",
                "- Bidirectional LSTM layers (128 → 64 units)\n",
                "- Regular LSTM layer (32 units)\n",
                "- Batch normalization for stable training\n",
                "- Progressive dropout (0.3 → 0.4) for regularization\n",
                "- Dense classification head (64 → 32 → 10)\n",
                "\n",
                "**Training Strategy:**\n",
                "- Adam optimizer with LR=0.001\n",
                "- ReduceLROnPlateau callback\n",
                "- Early stopping with patience=20\n",
                "- Batch size=64 for faster training\n",
                "\n",
                "**Expected Performance:**\n",
                "- Target accuracy: 75-85% (depending on feature quality)\n",
                "- Much faster than extracting features from audio\n",
                "- Better generalization with pre-extracted features"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "gtzan",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.25"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}