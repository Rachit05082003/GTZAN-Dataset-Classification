{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# GTZAN Genre Classification – CNN with Attention (Preprocess + Train)\n",
                "This notebook performs the full preprocessing of the GTZAN audio files (log‑Mel spectrogram + optional chromagram), creates train/validation splits, and then trains a simple CNN with a CBAM (Convolutional Block Attention Module) block.\n",
                "All required packages are installed in the first cell, so you can run the notebook on a fresh environment.\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "# Install required packages (run once)\n",
                "!pip install -q numpy librosa scikit-image torch torchvision tqdm scikit-learn\n"
            ],
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "import os, glob, numpy as np\n",
                "import librosa\n",
                "from skimage.transform import resize\n",
                "from sklearn.model_selection import train_test_split\n",
                "import torch, torch.nn as nn, torch.nn.functional as F, torch.optim as optim\n",
                "from torch.utils.data import TensorDataset, DataLoader\n",
                "from tqdm import tqdm\n"
            ],
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "# ---------------------------------------------------------------------------\n",
                "# Configuration – adjust paths if your folder layout differs\n",
                "# ---------------------------------------------------------------------------\n",
                "DATASET_ROOT = '/Users/narac0503/GIT/GTZAN Dataset Classification/GTZAN-Dataset-Classification/gtzan-classification/data/gtzan/genres_original'\n",
                "OUTPUT_DIR   = '/Users/narac0503/GIT/GTZAN Dataset Classification/GTZAN-Dataset-Classification/gtzan-classification/data/preprocessed'\n",
                "SAMPLE_RATE  = 22050\n",
                "DURATION     = 30.0  # seconds per clip\n",
                "N_MELS       = 128\n",
                "HOP_LENGTH   = 512\n",
                "IMG_SIZE     = (128, 128)  # (freq, time) after resizing\n",
                "USE_CHROMA   = True  # set False to use only log‑Mel\n",
                "os.makedirs(OUTPUT_DIR, exist_ok=True)\n"
            ],
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "def load_audio(path: str, sr: int = SAMPLE_RATE, duration: float = DURATION):\n",
                "    y, _ = librosa.load(path, sr=sr, duration=duration)\n",
                "    expected_len = int(sr * duration)\n",
                "    if len(y) < expected_len:\n",
                "        y = np.pad(y, (0, expected_len - len(y)))\n",
                "    else:\n",
                "        y = y[:expected_len]\n",
                "    return y\n",
                "\n",
                "def log_mel_spectrogram(y: np.ndarray, sr: int = SAMPLE_RATE):\n",
                "    S = librosa.feature.melspectrogram(y, sr=sr, n_mels=N_MELS, hop_length=HOP_LENGTH)\n",
                "    log_S = librosa.power_to_db(S, ref=np.max)\n",
                "    return log_S\n",
                "\n",
                "def chromagram(y: np.ndarray, sr: int = SAMPLE_RATE):\n",
                "    C = librosa.feature.chroma_stft(y=y, sr=sr, hop_length=HOP_LENGTH)\n",
                "    # Resize to same frequency dimension as mel (N_MELS)\n",
                "    C_resized = resize(C, (N_MELS, C.shape[1]), order=1, mode='constant', anti_aliasing=True)\n",
                "    return C_resized\n",
                "\n",
                "def preprocess_file(path: str):\n",
                "    y = load_audio(path)\n",
                "    mel = log_mel_spectrogram(y)\n",
                "    mel_resized = resize(mel, IMG_SIZE, order=1, mode='constant', anti_aliasing=True)\n",
                "    if USE_CHROMA:\n",
                "        chroma = chromagram(y)\n",
                "        chroma_resized = resize(chroma, IMG_SIZE, order=1, mode='constant', anti_aliasing=True)\n",
                "        img = np.stack([mel_resized, chroma_resized], axis=0)  # (C, H, W)\n",
                "    else:\n",
                "        img = mel_resized[np.newaxis, ...]\n",
                "    img = (img - img.mean()) / (img.std() + 1e-6)\n",
                "    return img.astype(np.float32)\n"
            ],
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "# ---------------------------------------------------------------------------\n",
                "# Load all audio files, compute spectrograms, and split into train/val\n",
                "# ---------------------------------------------------------------------------\n",
                "genres = sorted([d for d in os.listdir(DATASET_ROOT) if os.path.isdir(os.path.join(DATASET_ROOT, d))])\n",
                "label_map = {g: i for i, g in enumerate(genres)}\n",
                "print('Found genres:', label_map)\n",
                "\n",
                "X, y = [], []\n",
                "for genre in genres:\n",
                "    pattern = os.path.join(DATASET_ROOT, genre, '*.*')\n",
                "    for fp in glob.glob(pattern):\n",
                "        try:\n",
                "            img = preprocess_file(fp)\n",
                "            X.append(img)\n",
                "            y.append(label_map[genre])\n",
                "        except Exception as e:\n",
                "            print(f'[WARN] {fp}: {e}')\n",
                "\n",
                "X = np.stack(X)  # shape (N, C, H, W)\n",
                "y = np.array(y, dtype=np.int64)\n",
                "print(f'Processed {X.shape[0]} files – shape {X.shape}')\n",
                "\n",
                "# Stratified train/val split (80/20)\n",
                "X_train, X_val, y_train, y_val = train_test_split(\n",
                "    X, y, test_size=0.2, stratify=y, random_state=42)\n",
                "\n",
                "# Save to disk for later reuse\n",
                "np.savez_compressed(os.path.join(OUTPUT_DIR, 'train.npz'), X=X_train, y=y_train)\n",
                "np.savez_compressed(os.path.join(OUTPUT_DIR, 'val.npz'),   X=X_val,   y=y_val)\n",
                "print('Saved train.npz and val.npz to', OUTPUT_DIR)\n"
            ],
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "# ---------------------------------------------------------------------------\n",
                "# CBAM (Channel + Spatial Attention) implementation\n",
                "# ---------------------------------------------------------------------------\n",
                "class CBAM(nn.Module):\n",
                "    def __init__(self, channels, reduction=16):\n",
                "        super().__init__()\n",
                "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
                "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
                "        self.mlp = nn.Sequential(\n",
                "            nn.Conv2d(channels, channels // reduction, kernel_size=1, bias=False),\n",
                "            nn.ReLU(inplace=True),\n",
                "            nn.Conv2d(channels // reduction, channels, kernel_size=1, bias=False)\n",
                "        )\n",
                "        self.sigmoid = nn.Sigmoid()\n",
                "        self.conv_spatial = nn.Conv2d(2, 1, kernel_size=7, padding=3, bias=False)\n",
                "    def forward(self, x):\n",
                "        avg_out = self.mlp(self.avg_pool(x))\n",
                "        max_out = self.mlp(self.max_pool(x))\n",
                "        scale = self.sigmoid(avg_out + max_out)\n",
                "        x = x * scale\n",
                "        avg_pool = torch.mean(x, dim=1, keepdim=True)\n",
                "        max_pool, _ = torch.max(x, dim=1, keepdim=True)\n",
                "        concat = torch.cat([avg_pool, max_pool], dim=1)\n",
                "        scale_spatial = self.sigmoid(self.conv_spatial(concat))\n",
                "        out = x * scale_spatial\n",
                "        return out\n"
            ],
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "# ---------------------------------------------------------------------------\n",
                "# Simple CNN model that uses CBAM\n",
                "# ---------------------------------------------------------------------------\n",
                "class SimpleCNN(nn.Module):\n",
                "    def __init__(self, in_channels, num_classes, img_size=(128,128)):\n",
                "        super().__init__()\n",
                "        self.conv1 = nn.Conv2d(in_channels, 32, kernel_size=3, padding=1)\n",
                "        self.bn1   = nn.BatchNorm2d(32)\n",
                "        self.cbam1 = CBAM(32)\n",
                "        self.pool  = nn.MaxPool2d(2)\n",
                "        h, w = img_size\n",
                "        self.flat_dim = 32 * (h//2) * (w//2)\n",
                "        self.fc = nn.Linear(self.flat_dim, num_classes)\n",
                "    def forward(self, x):\n",
                "        x = F.relu(self.bn1(self.conv1(x)))\n",
                "        x = self.cbam1(x)\n",
                "        x = self.pool(x)\n",
                "        x = x.view(x.size(0), -1)\n",
                "        x = self.fc(x)\n",
                "        return x\n"
            ],
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "# ---------------------------------------------------------------------------\n",
                "# Prepare DataLoaders from the in‑memory arrays\n",
                "# ---------------------------------------------------------------------------\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "num_classes = len(np.unique(y_train))\n",
                "in_channels = X_train.shape[1]  # 1 or 2 depending on USE_CHROMA\n",
                "model = SimpleCNN(in_channels=in_channels, num_classes=num_classes).to(device)\n",
                "criterion = nn.CrossEntropyLoss()\n",
                "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
                "batch_size = 32\n",
                "train_ds = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n",
                "val_ds   = TensorDataset(torch.from_numpy(X_val),   torch.from_numpy(y_val))\n",
                "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
                "val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False, pin_memory=True)\n"
            ],
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "def train_one_epoch(loader, model, criterion, optimizer):\n",
                "    model.train()\n",
                "    running_loss = 0.0\n",
                "    correct = 0\n",
                "    total = 0\n",
                "    for xb, yb in tqdm(loader, leave=False):\n",
                "        xb, yb = xb.to(device), yb.to(device)\n",
                "        optimizer.zero_grad()\n",
                "        outputs = model(xb)\n",
                "        loss = criterion(outputs, yb)\n",
                "        loss.backward()\n",
                "        optimizer.step()\n",
                "        running_loss += loss.item() * xb.size(0)\n",
                "        _, preds = torch.max(outputs, 1)\n",
                "        correct += (preds == yb).sum().item()\n",
                "        total += xb.size(0)\n",
                "    return running_loss / total, correct / total\n",
                "\n",
                "def evaluate(loader, model, criterion):\n",
                "    model.eval()\n",
                "    running_loss = 0.0\n",
                "    correct = 0\n",
                "    total = 0\n",
                "    with torch.no_grad():\n",
                "        for xb, yb in loader:\n",
                "            xb, yb = xb.to(device), yb.to(device)\n",
                "            outputs = model(xb)\n",
                "            loss = criterion(outputs, yb)\n",
                "            running_loss += loss.item() * xb.size(0)\n",
                "            _, preds = torch.max(outputs, 1)\n",
                "            correct += (preds == yb).sum().item()\n",
                "            total += xb.size(0)\n",
                "    return running_loss / total, correct / total\n"
            ],
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "# ---------------------------------------------------------------------------\n",
                "# Training loop\n",
                "# ---------------------------------------------------------------------------\n",
                "best_val_acc = 0.0\n",
                "num_epochs = 30\n",
                "for epoch in range(1, num_epochs+1):\n",
                "    train_loss, train_acc = train_one_epoch(train_loader, model, criterion, optimizer)\n",
                "    val_loss,   val_acc   = evaluate(val_loader, model, criterion)\n",
                "    if val_acc > best_val_acc:\n",
                "        best_val_acc = val_acc\n",
                "        torch.save(model.state_dict(), 'best_cnn_attention.pth')\n",
                "    print(f'Epoch {epoch:02d} | Train loss {train_loss:.4f} acc {train_acc:.4f} | Val loss {val_loss:.4f} acc {val_acc:.4f}')\n",
                "\n",
                "print('Training complete – best validation accuracy:', best_val_acc)\n"
            ],
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "# Load best model and report final validation metrics\n",
                "model.load_state_dict(torch.load('best_cnn_attention.pth'))\n",
                "val_loss, val_acc = evaluate(val_loader, model, criterion)\n",
                "print(f'Final validation loss: {val_loss:.4f}, accuracy: {val_acc:.4f}')\n"
            ],
            "outputs": [],
            "execution_count": null
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "file_extension": ".py"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}