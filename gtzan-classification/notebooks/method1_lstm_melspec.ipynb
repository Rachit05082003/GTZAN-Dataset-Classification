{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Method 1: LSTM with Mel Spectrogram Preprocessing\n",
                "\n",
                "**Uses same preprocessing as CNN methods:**\n",
                "- Extract mel spectrograms directly from audio files\n",
                "- GroupShuffleSplit by song ID\n",
                "- Bidirectional LSTM architecture"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import librosa\n",
                "import os\n",
                "import tensorflow as tf\n",
                "from tensorflow.keras.models import Sequential\n",
                "from tensorflow.keras.layers import (LSTM, Bidirectional, Dense, Dropout, \n",
                "                                     BatchNormalization, GlobalAveragePooling1D)\n",
                "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
                "from tensorflow.keras.utils import to_categorical\n",
                "from sklearn.model_selection import GroupShuffleSplit\n",
                "from sklearn.preprocessing import LabelEncoder\n",
                "from sklearn.metrics import classification_report, accuracy_score\n",
                "import matplotlib.pyplot as plt\n",
                "from tqdm import tqdm\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "np.random.seed(42)\n",
                "tf.random.set_seed(42)\n",
                "\n",
                "print(f'TensorFlow: {tf.__version__}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "DATA_PATH = '../data/gtzan/genres_original'\n",
                "SAMPLE_RATE = 22050\n",
                "DURATION = 30\n",
                "N_MELS = 128\n",
                "N_FFT = 2048\n",
                "HOP_LENGTH = 512\n",
                "\n",
                "GENRES = ['blues', 'classical', 'country', 'disco', 'hiphop',\n",
                "          'jazz', 'metal', 'pop', 'reggae', 'rock']\n",
                "NUM_CLASSES = len(GENRES)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Feature Extraction (Same as CNN methods)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def extract_melspec(audio, sr):\n",
                "    \"\"\"\n",
                "    Extract mel spectrogram - same preprocessing as CNN methods.\n",
                "    Returns: (time_frames, n_mels) for LSTM input\n",
                "    \"\"\"\n",
                "    mel = librosa.feature.melspectrogram(\n",
                "        y=audio, sr=sr, n_mels=N_MELS, n_fft=N_FFT, hop_length=HOP_LENGTH\n",
                "    )\n",
                "    mel_db = librosa.power_to_db(mel, ref=np.max)\n",
                "    \n",
                "    # Transpose to (time, frequency) for LSTM\n",
                "    return mel_db.T\n",
                "\n",
                "# Test\n",
                "test_audio = np.random.randn(SAMPLE_RATE * 30)\n",
                "test_features = extract_melspec(test_audio, SAMPLE_RATE)\n",
                "print(f\"Feature shape: {test_features.shape}\")  # (time_frames, 128)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Load Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def load_dataset(data_path, target_length=1291):\n",
                "    \"\"\"\n",
                "    Load GTZAN and extract mel spectrograms.\n",
                "    \"\"\"\n",
                "    X, y, song_ids = [], [], []\n",
                "    \n",
                "    for genre in GENRES:\n",
                "        genre_path = os.path.join(data_path, genre)\n",
                "        if not os.path.exists(genre_path):\n",
                "            continue\n",
                "        \n",
                "        files = sorted([f for f in os.listdir(genre_path) if f.endswith('.wav')])\n",
                "        \n",
                "        for filename in tqdm(files, desc=f\"{genre}\"):\n",
                "            if 'jazz.00054' in filename:\n",
                "                continue\n",
                "            \n",
                "            filepath = os.path.join(genre_path, filename)\n",
                "            song_id = f\"{genre}.{filename.split('.')[1]}\"\n",
                "            \n",
                "            try:\n",
                "                audio, sr = librosa.load(filepath, sr=SAMPLE_RATE, duration=DURATION)\n",
                "                \n",
                "                # Pad to exact duration\n",
                "                target_samples = SAMPLE_RATE * DURATION\n",
                "                if len(audio) < target_samples:\n",
                "                    audio = np.pad(audio, (0, target_samples - len(audio)))\n",
                "                \n",
                "                # Extract mel spectrogram\n",
                "                melspec = extract_melspec(audio, sr)\n",
                "                \n",
                "                # Pad/truncate to target length\n",
                "                if melspec.shape[0] < target_length:\n",
                "                    pad_width = target_length - melspec.shape[0]\n",
                "                    melspec = np.pad(melspec, ((0, pad_width), (0, 0)))\n",
                "                else:\n",
                "                    melspec = melspec[:target_length, :]\n",
                "                \n",
                "                X.append(melspec)\n",
                "                y.append(genre)\n",
                "                song_ids.append(song_id)\n",
                "                \n",
                "            except Exception as e:\n",
                "                print(f\"Error: {filename}: {e}\")\n",
                "    \n",
                "    return np.array(X), np.array(y), np.array(song_ids)\n",
                "\n",
                "print(\"Loading dataset...\")\n",
                "X, y, song_ids = load_dataset(DATA_PATH)\n",
                "print(f\"\\nDataset shape: {X.shape}\")\n",
                "print(f\"Unique songs: {len(np.unique(song_ids))}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. GroupShuffleSplit by Song ID"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Encode labels\n",
                "label_encoder = LabelEncoder()\n",
                "y_encoded = label_encoder.fit_transform(y)\n",
                "y_cat = to_categorical(y_encoded, NUM_CLASSES)\n",
                "\n",
                "# Split by song: 80% train, 10% val, 10% test\n",
                "splitter = GroupShuffleSplit(test_size=0.20, n_splits=1, random_state=42)\n",
                "train_idx, test_idx = next(splitter.split(X, y, song_ids))\n",
                "\n",
                "X_train_full, X_test = X[train_idx], X[test_idx]\n",
                "y_train_full, y_test = y_cat[train_idx], y_cat[test_idx]\n",
                "songs_train = song_ids[train_idx]\n",
                "\n",
                "# Split train into train/val\n",
                "splitter_val = GroupShuffleSplit(test_size=0.125, n_splits=1, random_state=42)\n",
                "train_idx2, val_idx = next(splitter_val.split(X_train_full, y_train_full, songs_train))\n",
                "\n",
                "X_train = X_train_full[train_idx2]\n",
                "X_val = X_train_full[val_idx]\n",
                "y_train = y_train_full[train_idx2]\n",
                "y_val = y_train_full[val_idx]\n",
                "\n",
                "print(f\"Train: {X_train.shape}\")\n",
                "print(f\"Val: {X_val.shape}\")\n",
                "print(f\"Test: {X_test.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Normalization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Normalize using training statistics\n",
                "mean = X_train.mean()\n",
                "std = X_train.std()\n",
                "\n",
                "X_train = (X_train - mean) / (std + 1e-8)\n",
                "X_val = (X_val - mean) / (std + 1e-8)\n",
                "X_test = (X_test - mean) / (std + 1e-8)\n",
                "\n",
                "# Save normalization params for inference\n",
                "np.savez('../models/lstm_norm_params.npz', mean=mean, std=std)\n",
                "\n",
                "print(f\"LSTM input shape: {X_train.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Build LSTM Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def build_lstm_model(input_shape, num_classes):\n",
                "    model = Sequential([\n",
                "        # Bidirectional LSTM layers\n",
                "        Bidirectional(LSTM(128, return_sequences=True), input_shape=input_shape),\n",
                "        Dropout(0.3),\n",
                "        \n",
                "        Bidirectional(LSTM(64, return_sequences=True)),\n",
                "        Dropout(0.3),\n",
                "        \n",
                "        # Global pooling over time\n",
                "        GlobalAveragePooling1D(),\n",
                "        \n",
                "        # Dense layers\n",
                "        Dense(128, activation='relu'),\n",
                "        BatchNormalization(),\n",
                "        Dropout(0.4),\n",
                "        \n",
                "        Dense(64, activation='relu'),\n",
                "        Dropout(0.3),\n",
                "        \n",
                "        Dense(num_classes, activation='softmax')\n",
                "    ])\n",
                "    \n",
                "    model.compile(\n",
                "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
                "        loss='categorical_crossentropy',\n",
                "        metrics=['accuracy']\n",
                "    )\n",
                "    return model\n",
                "\n",
                "# Input shape: (time_steps, features) = (1291, 128)\n",
                "model = build_lstm_model((X_train.shape[1], X_train.shape[2]), NUM_CLASSES)\n",
                "model.summary()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Training"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "callbacks = [\n",
                "    EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True),\n",
                "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
                "]\n",
                "\n",
                "history = model.fit(\n",
                "    X_train, y_train,\n",
                "    validation_data=(X_val, y_val),\n",
                "    epochs=100,\n",
                "    batch_size=16,\n",
                "    callbacks=callbacks,\n",
                "    verbose=1\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Evaluation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot training history\n",
                "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
                "axes[0].plot(history.history['accuracy'], label='Train')\n",
                "axes[0].plot(history.history['val_accuracy'], label='Val')\n",
                "axes[0].set_title('Accuracy')\n",
                "axes[0].legend()\n",
                "\n",
                "axes[1].plot(history.history['loss'], label='Train')\n",
                "axes[1].plot(history.history['val_loss'], label='Val')\n",
                "axes[1].set_title('Loss')\n",
                "axes[1].legend()\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "# Evaluate\n",
                "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
                "print(f\"\\nTest Accuracy: {test_acc*100:.2f}%\")\n",
                "\n",
                "y_pred = model.predict(X_test)\n",
                "y_pred_classes = np.argmax(y_pred, axis=1)\n",
                "y_true_classes = np.argmax(y_test, axis=1)\n",
                "\n",
                "print(\"\\nClassification Report:\")\n",
                "print(classification_report(y_true_classes, y_pred_classes, target_names=label_encoder.classes_))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Save Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model.save('../models/lstm_melspec.keras')\n",
                "print(\"Model saved to ../models/lstm_melspec.keras\")\n",
                "print(\"Normalization params saved to ../models/lstm_norm_params.npz\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.11.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}