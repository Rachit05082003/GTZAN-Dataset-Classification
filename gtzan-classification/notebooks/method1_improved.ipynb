{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Method 1 Improved: Dense Neural Network with Correct Data Split\n",
                "\n",
                "**Improvements over original LSTM:**\n",
                "- **Architecture**: Replaced LSTM with a Dense Neural Network (MLP). LSTM is designed for sequential data, but the CSV features (mean/variance) are tabular summary statistics, not a time sequence. An MLP is more appropriate and effective for this data type.\n",
                "- **Data Splitting**: Implemented `GroupShuffleSplit` based on Song ID. The original method split randomly, allowing 3-second segments of the *same song* to appear in both Training and Validation sets (Data Leakage). This ensures we evaluate true generalization to new unseen songs.\n",
                "- **Evaluation**: We evaluate on a strictly held-out test set of songs, using both segment-level accuracy and **Song-Level Majority Voting** (aggregating predictions for a song to get the final label)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "import tensorflow as tf\n",
                "from tensorflow.keras.models import Sequential\n",
                "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Activation\n",
                "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
                "from tensorflow.keras.utils import to_categorical\n",
                "from sklearn.model_selection import GroupShuffleSplit\n",
                "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
                "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# Reproducibility\n",
                "np.random.seed(42)\n",
                "tf.random.set_seed(42)\n",
                "\n",
                "print(f'TensorFlow version: {tf.__version__}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Load and Prepare Data\n",
                "We use the 3-second features dataset. We parse the filename to get the Song ID for grouping."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load 3-second features\n",
                "df = pd.read_csv('../data/gtzan/features_3_sec.csv')\n",
                "\n",
                "# Extract Song ID (e.g., 'blues.00000') to group segments\n",
                "# Filename format: genre.00000.0.wav\n",
                "df['song_id'] = df['filename'].apply(lambda x: '.'.join(x.split('.')[:2]))\n",
                "\n",
                "print(f\"Total segments: {len(df)}\")\n",
                "print(f\"Total unique songs: {df['song_id'].nunique()}\")\n",
                "print(f\"Genres: {df['label'].unique()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Proper Train/Val/Test Split\n",
                "We split by **Song ID** so all segments of a song stay together. This prevents the model from \"memorizing\" a song from one segment and recognizing it in the validation set."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define features and label\n",
                "X = df.drop(columns=['filename', 'length', 'label', 'song_id'])\n",
                "y = df['label']\n",
                "groups = df['song_id']\n",
                "\n",
                "# Encode labels\n",
                "label_encoder = LabelEncoder()\n",
                "y_encoded = label_encoder.fit_transform(y)\n",
                "num_classes = len(label_encoder.classes_)\n",
                "y_cat = to_categorical(y_encoded, num_classes)\n",
                "\n",
                "# Split: 70% Train, 15% Val, 15% Test (approx)\n",
                "# First split: Train vs (Val + Test)\n",
                "splitter_outer = GroupShuffleSplit(test_size=0.30, n_splits=1, random_state=42)\n",
                "train_idx, temp_idx = next(splitter_outer.split(X, y, groups))\n",
                "\n",
                "X_train, X_temp = X.iloc[train_idx], X.iloc[temp_idx]\n",
                "y_train, y_temp = y_cat[train_idx], y_cat[temp_idx]\n",
                "groups_temp = groups.iloc[temp_idx]\n",
                "\n",
                "# Second split: Val vs Test\n",
                "splitter_inner = GroupShuffleSplit(test_size=0.50, n_splits=1, random_state=42)\n",
                "val_idx, test_idx = next(splitter_inner.split(X_temp, y_temp, groups_temp))\n",
                "\n",
                "X_val, X_test = X_temp.iloc[val_idx], X_temp.iloc[test_idx]\n",
                "y_val, y_test = y_temp[val_idx], y_temp[test_idx]\n",
                "groups_test = groups_temp.iloc[test_idx]\n",
                "\n",
                "print(f\"Training set shape: {X_train.shape}\")\n",
                "print(f\"Validation set shape: {X_val.shape}\")\n",
                "print(f\"Test set shape: {X_test.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Standardization\n",
                "Standardize features based on Training statistics."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "scaler = StandardScaler()\n",
                "X_train_scaled = scaler.fit_transform(X_train)\n",
                "X_val_scaled = scaler.transform(X_val)\n",
                "X_test_scaled = scaler.transform(X_test)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Build Dense Network (MLP)\n",
                "We use a deep dense network with Dropout and Batch Normalization for regularization."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def build_model(input_shape, num_classes):\n",
                "    model = Sequential([\n",
                "        # Input Layer\n",
                "        Dense(512, input_shape=input_shape),\n",
                "        BatchNormalization(),\n",
                "        Activation('relu'),\n",
                "        Dropout(0.3),\n",
                "\n",
                "        # Hidden Layer 1\n",
                "        Dense(256),\n",
                "        BatchNormalization(),\n",
                "        Activation('relu'),\n",
                "        Dropout(0.3),\n",
                "\n",
                "        # Hidden Layer 2\n",
                "        Dense(128),\n",
                "        BatchNormalization(),\n",
                "        Activation('relu'),\n",
                "        Dropout(0.3),\n",
                "        \n",
                "        # Hidden Layer 3\n",
                "        Dense(64),\n",
                "        BatchNormalization(),\n",
                "        Activation('relu'),\n",
                "        Dropout(0.3),\n",
                "\n",
                "        # Output Layer\n",
                "        Dense(num_classes, activation='softmax')\n",
                "    ])\n",
                "    \n",
                "    model.compile(\n",
                "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
                "        loss='categorical_crossentropy',\n",
                "        metrics=['accuracy']\n",
                "    )\n",
                "    return model\n",
                "\n",
                "model = build_model((X_train.shape[1],), num_classes)\n",
                "model.summary()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Training"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "callbacks = [\n",
                "    EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True),\n",
                "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
                "]\n",
                "\n",
                "history = model.fit(\n",
                "    X_train_scaled, y_train,\n",
                "    validation_data=(X_val_scaled, y_val),\n",
                "    epochs=100,\n",
                "    batch_size=64,\n",
                "    callbacks=callbacks,\n",
                "    verbose=1\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Segment-Level Evaluation\n",
                "Evaluation on 3-second segments."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot accuracy and loss\n",
                "plt.figure(figsize=(12, 4))\n",
                "plt.subplot(1, 2, 1)\n",
                "plt.plot(history.history['accuracy'], label='Train')\n",
                "plt.plot(history.history['val_accuracy'], label='Val')\n",
                "plt.title('Accuracy')\n",
                "plt.legend()\n",
                "\n",
                "plt.subplot(1, 2, 2)\n",
                "plt.plot(history.history['loss'], label='Train')\n",
                "plt.plot(history.history['val_loss'], label='Val')\n",
                "plt.title('Loss')\n",
                "plt.legend()\n",
                "plt.show()\n",
                "\n",
                "# Evaluate on Test Set\n",
                "test_loss, test_acc = model.evaluate(X_test_scaled, y_test, verbose=0)\n",
                "print(f\"\\nTest Accuracy (on 3-second segments): {test_acc*100:.2f}%\")\n",
                "\n",
                "# Confusion Matrix\n",
                "y_pred_prob = model.predict(X_test_scaled)\n",
                "y_pred_classes = np.argmax(y_pred_prob, axis=1)\n",
                "y_true_classes = np.argmax(y_test, axis=1)\n",
                "\n",
                "print(\"\\nSegment-Level Classification Report:\")\n",
                "print(classification_report(y_true_classes, y_pred_classes, target_names=label_encoder.classes_))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Song-Level Evaluation (Majority Voting)\n",
                "We aggregate predictions for all segments of a song and take the majority class. This is the true metric for music classification."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create a DataFrame with true labels, predicted labels, and song IDs\n",
                "results_df = pd.DataFrame({\n",
                "    'song_id': groups_test.values,\n",
                "    'true_label': y_true_classes,\n",
                "    'pred_label': y_pred_classes\n",
                "})\n",
                "\n",
                "# Aggregate by song_id using mode (majority vote)\n",
                "# If multiple modes, it picks the first one (acceptable)\n",
                "song_results = results_df.groupby('song_id').agg(lambda x: x.mode()[0])\n",
                "\n",
                "song_acc = accuracy_score(song_results['true_label'], song_results['pred_label'])\n",
                "print(f\"\\nSong-Level Accuracy: {song_acc*100:.2f}%\")\n",
                "\n",
                "print(\"\\nSong-Level Classification Report:\")\n",
                "print(classification_report(song_results['true_label'], song_results['pred_label'], target_names=label_encoder.classes_))"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}