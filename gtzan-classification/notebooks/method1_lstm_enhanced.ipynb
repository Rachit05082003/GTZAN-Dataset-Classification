{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Method 1 Improved: LSTM with Enhanced Features\n",
                "\n",
                "**Key Improvements:**\n",
                "- **GroupShuffleSplit by Song ID**: Prevents data leakage by keeping all segments of a song together\n",
                "- **Enhanced Feature Engineering**: Delta MFCCs, spectral contrast, tonnetz, chroma CQT\n",
                "- **Better Architecture**: Bidirectional LSTM with attention-like pooling"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "import librosa\n",
                "import os\n",
                "import tensorflow as tf\n",
                "from tensorflow.keras.models import Sequential, Model\n",
                "from tensorflow.keras.layers import (LSTM, Bidirectional, Dense, Dropout, \n",
                "                                     BatchNormalization, Input, GlobalAveragePooling1D)\n",
                "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
                "from tensorflow.keras.utils import to_categorical\n",
                "from sklearn.model_selection import GroupShuffleSplit\n",
                "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
                "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from tqdm import tqdm\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "np.random.seed(42)\n",
                "tf.random.set_seed(42)\n",
                "\n",
                "print(f'TensorFlow: {tf.__version__}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "DATA_PATH = '../data/gtzan/genres_original'\n",
                "SAMPLE_RATE = 22050\n",
                "DURATION = 3  # seconds per segment\n",
                "SAMPLES_PER_SEGMENT = SAMPLE_RATE * DURATION\n",
                "\n",
                "GENRES = ['blues', 'classical', 'country', 'disco', 'hiphop',\n",
                "          'jazz', 'metal', 'pop', 'reggae', 'rock']\n",
                "NUM_CLASSES = len(GENRES)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Enhanced Feature Extraction\n",
                "We extract more features than the basic CSV, including delta MFCCs and tonal features."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def extract_enhanced_features(audio, sr):\n",
                "    \"\"\"\n",
                "    Extract enhanced audio features including:\n",
                "    - MFCCs (20) + Delta + Delta-Delta\n",
                "    - Chroma (CQT-based)\n",
                "    - Spectral Contrast\n",
                "    - Tonnetz\n",
                "    - Spectral features (centroid, bandwidth, rolloff, flatness)\n",
                "    - Zero crossing rate\n",
                "    - RMS energy\n",
                "    \"\"\"\n",
                "    features = []\n",
                "    \n",
                "    # MFCCs (20 coefficients)\n",
                "    mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=20)\n",
                "    mfcc_delta = librosa.feature.delta(mfcc)\n",
                "    mfcc_delta2 = librosa.feature.delta(mfcc, order=2)\n",
                "    \n",
                "    for i in range(20):\n",
                "        features.extend([mfcc[i].mean(), mfcc[i].std()])\n",
                "        features.extend([mfcc_delta[i].mean(), mfcc_delta[i].std()])\n",
                "        features.extend([mfcc_delta2[i].mean(), mfcc_delta2[i].std()])\n",
                "    \n",
                "    # Chroma CQT (12 bins)\n",
                "    chroma = librosa.feature.chroma_cqt(y=audio, sr=sr)\n",
                "    for i in range(12):\n",
                "        features.extend([chroma[i].mean(), chroma[i].std()])\n",
                "    \n",
                "    # Spectral Contrast (7 bands)\n",
                "    contrast = librosa.feature.spectral_contrast(y=audio, sr=sr)\n",
                "    for i in range(7):\n",
                "        features.extend([contrast[i].mean(), contrast[i].std()])\n",
                "    \n",
                "    # Tonnetz (6 dimensions)\n",
                "    tonnetz = librosa.feature.tonnetz(y=librosa.effects.harmonic(audio), sr=sr)\n",
                "    for i in range(6):\n",
                "        features.extend([tonnetz[i].mean(), tonnetz[i].std()])\n",
                "    \n",
                "    # Spectral features\n",
                "    cent = librosa.feature.spectral_centroid(y=audio, sr=sr)\n",
                "    bw = librosa.feature.spectral_bandwidth(y=audio, sr=sr)\n",
                "    rolloff = librosa.feature.spectral_rolloff(y=audio, sr=sr)\n",
                "    flatness = librosa.feature.spectral_flatness(y=audio)\n",
                "    \n",
                "    features.extend([cent.mean(), cent.std()])\n",
                "    features.extend([bw.mean(), bw.std()])\n",
                "    features.extend([rolloff.mean(), rolloff.std()])\n",
                "    features.extend([flatness.mean(), flatness.std()])\n",
                "    \n",
                "    # Zero crossing rate\n",
                "    zcr = librosa.feature.zero_crossing_rate(y=audio)\n",
                "    features.extend([zcr.mean(), zcr.std()])\n",
                "    \n",
                "    # RMS energy\n",
                "    rms = librosa.feature.rms(y=audio)\n",
                "    features.extend([rms.mean(), rms.std()])\n",
                "    \n",
                "    # Tempo\n",
                "    tempo, _ = librosa.beat.beat_track(y=audio, sr=sr)\n",
                "    features.append(float(tempo))\n",
                "    \n",
                "    return np.array(features)\n",
                "\n",
                "# Test feature extraction\n",
                "test_audio = np.random.randn(SAMPLES_PER_SEGMENT)\n",
                "test_features = extract_enhanced_features(test_audio, SAMPLE_RATE)\n",
                "print(f\"Feature vector size: {len(test_features)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Load Dataset with Enhanced Features"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def load_dataset(data_path, segment_duration=3):\n",
                "    \"\"\"\n",
                "    Load GTZAN dataset and extract enhanced features for each segment.\n",
                "    Returns features, labels, and song_ids for grouping.\n",
                "    \"\"\"\n",
                "    X, y, song_ids = [], [], []\n",
                "    samples_per_segment = SAMPLE_RATE * segment_duration\n",
                "    \n",
                "    for genre_idx, genre in enumerate(GENRES):\n",
                "        genre_path = os.path.join(data_path, genre)\n",
                "        if not os.path.exists(genre_path):\n",
                "            print(f\"Warning: {genre_path} not found\")\n",
                "            continue\n",
                "            \n",
                "        files = sorted([f for f in os.listdir(genre_path) if f.endswith('.wav')])\n",
                "        \n",
                "        for filename in tqdm(files, desc=f\"{genre}\"):\n",
                "            if 'jazz.00054' in filename:  # Skip corrupted file\n",
                "                continue\n",
                "                \n",
                "            filepath = os.path.join(genre_path, filename)\n",
                "            song_id = f\"{genre}.{filename.split('.')[1]}\"  # e.g., 'blues.00000'\n",
                "            \n",
                "            try:\n",
                "                audio, sr = librosa.load(filepath, sr=SAMPLE_RATE, duration=30)\n",
                "                \n",
                "                # Split into segments\n",
                "                num_segments = len(audio) // samples_per_segment\n",
                "                \n",
                "                for seg_idx in range(num_segments):\n",
                "                    start = seg_idx * samples_per_segment\n",
                "                    end = start + samples_per_segment\n",
                "                    segment = audio[start:end]\n",
                "                    \n",
                "                    if len(segment) == samples_per_segment:\n",
                "                        features = extract_enhanced_features(segment, sr)\n",
                "                        X.append(features)\n",
                "                        y.append(genre)\n",
                "                        song_ids.append(song_id)\n",
                "                        \n",
                "            except Exception as e:\n",
                "                print(f\"Error: {filename}: {e}\")\n",
                "    \n",
                "    return np.array(X), np.array(y), np.array(song_ids)\n",
                "\n",
                "print(\"Loading dataset with enhanced features...\")\n",
                "X, y, song_ids = load_dataset(DATA_PATH)\n",
                "print(f\"\\nDataset: {X.shape[0]} samples, {X.shape[1]} features\")\n",
                "print(f\"Unique songs: {len(np.unique(song_ids))}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. GroupShuffleSplit by Song ID\n",
                "This ensures ALL segments from a song stay in the same split, preventing data leakage."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Encode labels\n",
                "label_encoder = LabelEncoder()\n",
                "y_encoded = label_encoder.fit_transform(y)\n",
                "y_cat = to_categorical(y_encoded, NUM_CLASSES)\n",
                "\n",
                "# Split: 70% Train, 15% Val, 15% Test (by song)\n",
                "splitter_outer = GroupShuffleSplit(test_size=0.30, n_splits=1, random_state=42)\n",
                "train_idx, temp_idx = next(splitter_outer.split(X, y, song_ids))\n",
                "\n",
                "X_train, X_temp = X[train_idx], X[temp_idx]\n",
                "y_train, y_temp = y_cat[train_idx], y_cat[temp_idx]\n",
                "songs_temp = song_ids[temp_idx]\n",
                "\n",
                "splitter_inner = GroupShuffleSplit(test_size=0.50, n_splits=1, random_state=42)\n",
                "val_idx, test_idx = next(splitter_inner.split(X_temp, y_temp, songs_temp))\n",
                "\n",
                "X_val, X_test = X_temp[val_idx], X_temp[test_idx]\n",
                "y_val, y_test = y_temp[val_idx], y_temp[test_idx]\n",
                "songs_test = songs_temp[test_idx]\n",
                "\n",
                "print(f\"Train: {X_train.shape}\")\n",
                "print(f\"Val: {X_val.shape}\")\n",
                "print(f\"Test: {X_test.shape}\")\n",
                "\n",
                "# Verify no overlap\n",
                "train_songs = set(song_ids[train_idx])\n",
                "val_songs = set(songs_temp[val_idx])\n",
                "test_songs = set(songs_test)\n",
                "print(f\"\\nSong overlap check:\")\n",
                "print(f\"  Train-Val overlap: {len(train_songs & val_songs)}\")\n",
                "print(f\"  Train-Test overlap: {len(train_songs & test_songs)}\")\n",
                "print(f\"  Val-Test overlap: {len(val_songs & test_songs)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Standardization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "scaler = StandardScaler()\n",
                "X_train_scaled = scaler.fit_transform(X_train)\n",
                "X_val_scaled = scaler.transform(X_val)\n",
                "X_test_scaled = scaler.transform(X_test)\n",
                "\n",
                "# Reshape for LSTM: (samples, timesteps, features)\n",
                "# Treat feature dimension as sequence\n",
                "n_features = X_train_scaled.shape[1]\n",
                "X_train_lstm = X_train_scaled.reshape(-1, n_features, 1)\n",
                "X_val_lstm = X_val_scaled.reshape(-1, n_features, 1)\n",
                "X_test_lstm = X_test_scaled.reshape(-1, n_features, 1)\n",
                "\n",
                "print(f\"LSTM input shape: {X_train_lstm.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Build Improved LSTM Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def build_lstm_model(input_shape, num_classes):\n",
                "    model = Sequential([\n",
                "        Bidirectional(LSTM(128, return_sequences=True), input_shape=input_shape),\n",
                "        Dropout(0.3),\n",
                "        Bidirectional(LSTM(64, return_sequences=True)),\n",
                "        Dropout(0.3),\n",
                "        GlobalAveragePooling1D(),\n",
                "        Dense(128, activation='relu'),\n",
                "        BatchNormalization(),\n",
                "        Dropout(0.4),\n",
                "        Dense(64, activation='relu'),\n",
                "        Dropout(0.3),\n",
                "        Dense(num_classes, activation='softmax')\n",
                "    ])\n",
                "    \n",
                "    model.compile(\n",
                "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
                "        loss='categorical_crossentropy',\n",
                "        metrics=['accuracy']\n",
                "    )\n",
                "    return model\n",
                "\n",
                "model = build_lstm_model((n_features, 1), NUM_CLASSES)\n",
                "model.summary()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Training"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "callbacks = [\n",
                "    EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True),\n",
                "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
                "]\n",
                "\n",
                "history = model.fit(\n",
                "    X_train_lstm, y_train,\n",
                "    validation_data=(X_val_lstm, y_val),\n",
                "    epochs=100,\n",
                "    batch_size=64,\n",
                "    callbacks=callbacks,\n",
                "    verbose=1\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Evaluation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot training history\n",
                "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
                "axes[0].plot(history.history['accuracy'], label='Train')\n",
                "axes[0].plot(history.history['val_accuracy'], label='Val')\n",
                "axes[0].set_title('Accuracy')\n",
                "axes[0].legend()\n",
                "\n",
                "axes[1].plot(history.history['loss'], label='Train')\n",
                "axes[1].plot(history.history['val_loss'], label='Val')\n",
                "axes[1].set_title('Loss')\n",
                "axes[1].legend()\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "# Segment-level accuracy\n",
                "test_loss, test_acc = model.evaluate(X_test_lstm, y_test, verbose=0)\n",
                "print(f\"\\nSegment Test Accuracy: {test_acc*100:.2f}%\")\n",
                "\n",
                "# Song-level accuracy (majority voting)\n",
                "y_pred_prob = model.predict(X_test_lstm)\n",
                "y_pred_classes = np.argmax(y_pred_prob, axis=1)\n",
                "y_true_classes = np.argmax(y_test, axis=1)\n",
                "\n",
                "results_df = pd.DataFrame({\n",
                "    'song_id': songs_test,\n",
                "    'true_label': y_true_classes,\n",
                "    'pred_label': y_pred_classes\n",
                "})\n",
                "\n",
                "song_results = results_df.groupby('song_id').agg(lambda x: x.mode()[0])\n",
                "song_acc = accuracy_score(song_results['true_label'], song_results['pred_label'])\n",
                "print(f\"Song-Level Accuracy: {song_acc*100:.2f}%\")\n",
                "\n",
                "print(\"\\nClassification Report:\")\n",
                "print(classification_report(y_true_classes, y_pred_classes, target_names=label_encoder.classes_))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Save Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model.save('../models/lstm_enhanced.keras')\n",
                "print(\"Model saved to ../models/lstm_enhanced.keras\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.11.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}