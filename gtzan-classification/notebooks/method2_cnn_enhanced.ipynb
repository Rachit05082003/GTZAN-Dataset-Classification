{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Method 2 Improved: CNN with Enhanced Preprocessing\n",
                "\n",
                "**Key Improvements:**\n",
                "- **GroupShuffleSplit by Song**: Prevents data leakage\n",
                "- **SpecAugment**: Time and frequency masking for data augmentation\n",
                "- **Multi-resolution features**: Stacked mel spectrograms at different resolutions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import librosa\n",
                "import os\n",
                "import tensorflow as tf\n",
                "from tensorflow.keras.models import Sequential\n",
                "from tensorflow.keras.layers import (Conv2D, BatchNormalization, MaxPooling2D,\n",
                "                                     GlobalAveragePooling2D, Dense, Dropout, LeakyReLU)\n",
                "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
                "from tensorflow.keras.utils import to_categorical\n",
                "from sklearn.model_selection import GroupShuffleSplit\n",
                "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
                "from sklearn.metrics import classification_report, accuracy_score\n",
                "import matplotlib.pyplot as plt\n",
                "from tqdm import tqdm\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "np.random.seed(42)\n",
                "tf.random.set_seed(42)\n",
                "\n",
                "print(f'TensorFlow: {tf.__version__}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "DATA_PATH = '../data/gtzan/genres_original'\n",
                "SAMPLE_RATE = 22050\n",
                "DURATION = 30\n",
                "N_MELS = 128\n",
                "N_FFT = 2048\n",
                "HOP_LENGTH = 512\n",
                "TARGET_LENGTH = 1291  # ~30 seconds at hop_length=512\n",
                "\n",
                "GENRES = ['blues', 'classical', 'country', 'disco', 'hiphop',\n",
                "          'jazz', 'metal', 'pop', 'reggae', 'rock']\n",
                "NUM_CLASSES = len(GENRES)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Enhanced Feature Extraction with Multi-Resolution"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def extract_melspec(audio, sr, n_mels=128, target_length=TARGET_LENGTH):\n",
                "    \"\"\"Extract mel spectrogram with padding/truncation.\"\"\"\n",
                "    mel = librosa.feature.melspectrogram(\n",
                "        y=audio, sr=sr, n_mels=n_mels, n_fft=N_FFT, hop_length=HOP_LENGTH\n",
                "    )\n",
                "    mel_db = librosa.power_to_db(mel, ref=np.max)\n",
                "    \n",
                "    # Transpose to (time, frequency)\n",
                "    mel_db = mel_db.T\n",
                "    \n",
                "    # Pad or truncate\n",
                "    if mel_db.shape[0] < target_length:\n",
                "        pad_width = target_length - mel_db.shape[0]\n",
                "        mel_db = np.pad(mel_db, ((0, pad_width), (0, 0)), mode='constant')\n",
                "    else:\n",
                "        mel_db = mel_db[:target_length, :]\n",
                "    \n",
                "    return mel_db\n",
                "\n",
                "def extract_multi_resolution(audio, sr):\n",
                "    \"\"\"\n",
                "    Extract multi-resolution features:\n",
                "    - Mel spectrogram (128 mels)\n",
                "    - Delta (first derivative)\n",
                "    - Delta-delta (second derivative)\n",
                "    Returns: (time, freq, 3) tensor\n",
                "    \"\"\"\n",
                "    mel = extract_melspec(audio, sr, n_mels=128)\n",
                "    \n",
                "    # Compute deltas along time axis\n",
                "    delta = librosa.feature.delta(mel.T).T\n",
                "    delta2 = librosa.feature.delta(mel.T, order=2).T\n",
                "    \n",
                "    # Stack as channels\n",
                "    return np.stack([mel, delta, delta2], axis=-1)\n",
                "\n",
                "# Test\n",
                "test_audio = np.random.randn(SAMPLE_RATE * 30)\n",
                "test_features = extract_multi_resolution(test_audio, SAMPLE_RATE)\n",
                "print(f\"Feature shape: {test_features.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. SpecAugment Data Augmentation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def spec_augment(mel, time_mask_param=80, freq_mask_param=20, num_masks=2):\n",
                "    \"\"\"\n",
                "    Apply SpecAugment: time and frequency masking.\n",
                "    \"\"\"\n",
                "    augmented = mel.copy()\n",
                "    time_steps, freq_bins = augmented.shape[:2]\n",
                "    \n",
                "    # Time masking\n",
                "    for _ in range(num_masks):\n",
                "        t = np.random.randint(0, time_mask_param)\n",
                "        t0 = np.random.randint(0, max(1, time_steps - t))\n",
                "        augmented[t0:t0+t, :] = 0\n",
                "    \n",
                "    # Frequency masking\n",
                "    for _ in range(num_masks):\n",
                "        f = np.random.randint(0, freq_mask_param)\n",
                "        f0 = np.random.randint(0, max(1, freq_bins - f))\n",
                "        augmented[:, f0:f0+f] = 0\n",
                "    \n",
                "    return augmented"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Load Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def load_dataset(data_path, augment=False):\n",
                "    X, y, song_ids = [], [], []\n",
                "    \n",
                "    for genre in GENRES:\n",
                "        genre_path = os.path.join(data_path, genre)\n",
                "        if not os.path.exists(genre_path):\n",
                "            continue\n",
                "            \n",
                "        files = sorted([f for f in os.listdir(genre_path) if f.endswith('.wav')])\n",
                "        \n",
                "        for filename in tqdm(files, desc=f\"{genre}\"):\n",
                "            if 'jazz.00054' in filename:\n",
                "                continue\n",
                "                \n",
                "            filepath = os.path.join(genre_path, filename)\n",
                "            song_id = f\"{genre}.{filename.split('.')[1]}\"\n",
                "            \n",
                "            try:\n",
                "                audio, sr = librosa.load(filepath, sr=SAMPLE_RATE, duration=DURATION)\n",
                "                target_len = SAMPLE_RATE * DURATION\n",
                "                if len(audio) < target_len:\n",
                "                    audio = np.pad(audio, (0, target_len - len(audio)))\n",
                "                \n",
                "                # Extract features\n",
                "                features = extract_multi_resolution(audio, sr)\n",
                "                X.append(features)\n",
                "                y.append(genre)\n",
                "                song_ids.append(song_id)\n",
                "                \n",
                "                # Augmentation\n",
                "                if augment:\n",
                "                    aug_features = spec_augment(features)\n",
                "                    X.append(aug_features)\n",
                "                    y.append(genre)\n",
                "                    song_ids.append(song_id)\n",
                "                    \n",
                "            except Exception as e:\n",
                "                print(f\"Error: {filename}: {e}\")\n",
                "    \n",
                "    return np.array(X), np.array(y), np.array(song_ids)\n",
                "\n",
                "print(\"Loading dataset...\")\n",
                "X, y, song_ids = load_dataset(DATA_PATH, augment=True)\n",
                "print(f\"\\nDataset: {X.shape}\")\n",
                "print(f\"Unique songs: {len(np.unique(song_ids))}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. GroupShuffleSplit by Song ID"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Encode labels\n",
                "label_encoder = LabelEncoder()\n",
                "y_encoded = label_encoder.fit_transform(y)\n",
                "y_cat = to_categorical(y_encoded, NUM_CLASSES)\n",
                "\n",
                "# Split by song\n",
                "splitter = GroupShuffleSplit(test_size=0.20, n_splits=1, random_state=42)\n",
                "train_idx, test_idx = next(splitter.split(X, y, song_ids))\n",
                "\n",
                "X_train, X_test = X[train_idx], X[test_idx]\n",
                "y_train, y_test = y_cat[train_idx], y_cat[test_idx]\n",
                "\n",
                "# Further split train into train/val\n",
                "songs_train = song_ids[train_idx]\n",
                "splitter_val = GroupShuffleSplit(test_size=0.125, n_splits=1, random_state=42)\n",
                "train_idx2, val_idx = next(splitter_val.split(X_train, y_train, songs_train))\n",
                "\n",
                "X_val = X_train[val_idx]\n",
                "y_val = y_train[val_idx]\n",
                "X_train = X_train[train_idx2]\n",
                "y_train = y_train[train_idx2]\n",
                "\n",
                "print(f\"Train: {X_train.shape}\")\n",
                "print(f\"Val: {X_val.shape}\")\n",
                "print(f\"Test: {X_test.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Normalization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Normalize per-channel\n",
                "mean = X_train.mean(axis=(0, 1, 2), keepdims=True)\n",
                "std = X_train.std(axis=(0, 1, 2), keepdims=True) + 1e-8\n",
                "\n",
                "X_train = (X_train - mean) / std\n",
                "X_val = (X_val - mean) / std\n",
                "X_test = (X_test - mean) / std\n",
                "\n",
                "print(f\"Normalized shape: {X_train.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Build CNN Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def build_cnn_model(input_shape, num_classes):\n",
                "    model = Sequential([\n",
                "        # Block 1\n",
                "        Conv2D(64, (3, 3), padding='same', input_shape=input_shape),\n",
                "        LeakyReLU(0.1),\n",
                "        BatchNormalization(),\n",
                "        MaxPooling2D((2, 2)),\n",
                "        \n",
                "        # Block 2\n",
                "        Conv2D(128, (3, 3), padding='same'),\n",
                "        LeakyReLU(0.1),\n",
                "        BatchNormalization(),\n",
                "        MaxPooling2D((2, 2)),\n",
                "        \n",
                "        # Block 3\n",
                "        Conv2D(256, (3, 3), padding='same'),\n",
                "        LeakyReLU(0.1),\n",
                "        BatchNormalization(),\n",
                "        MaxPooling2D((2, 2)),\n",
                "        \n",
                "        # Block 4\n",
                "        Conv2D(512, (3, 3), padding='same'),\n",
                "        LeakyReLU(0.1),\n",
                "        BatchNormalization(),\n",
                "        GlobalAveragePooling2D(),\n",
                "        \n",
                "        # Classifier\n",
                "        Dense(256),\n",
                "        LeakyReLU(0.1),\n",
                "        Dropout(0.5),\n",
                "        Dense(num_classes, activation='softmax')\n",
                "    ])\n",
                "    \n",
                "    model.compile(\n",
                "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
                "        loss='categorical_crossentropy',\n",
                "        metrics=['accuracy']\n",
                "    )\n",
                "    return model\n",
                "\n",
                "model = build_cnn_model(X_train.shape[1:], NUM_CLASSES)\n",
                "model.summary()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Training"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "callbacks = [\n",
                "    EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True),\n",
                "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
                "]\n",
                "\n",
                "history = model.fit(\n",
                "    X_train, y_train,\n",
                "    validation_data=(X_val, y_val),\n",
                "    epochs=100,\n",
                "    batch_size=16,\n",
                "    callbacks=callbacks,\n",
                "    verbose=1\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Evaluation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot\n",
                "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
                "axes[0].plot(history.history['accuracy'], label='Train')\n",
                "axes[0].plot(history.history['val_accuracy'], label='Val')\n",
                "axes[0].set_title('Accuracy')\n",
                "axes[0].legend()\n",
                "\n",
                "axes[1].plot(history.history['loss'], label='Train')\n",
                "axes[1].plot(history.history['val_loss'], label='Val')\n",
                "axes[1].set_title('Loss')\n",
                "axes[1].legend()\n",
                "plt.show()\n",
                "\n",
                "# Evaluate\n",
                "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
                "print(f\"\\nTest Accuracy: {test_acc*100:.2f}%\")\n",
                "\n",
                "y_pred = model.predict(X_test)\n",
                "y_pred_classes = np.argmax(y_pred, axis=1)\n",
                "y_true_classes = np.argmax(y_test, axis=1)\n",
                "\n",
                "print(\"\\nClassification Report:\")\n",
                "print(classification_report(y_true_classes, y_pred_classes, target_names=label_encoder.classes_))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. Save Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model.save('../models/cnn_enhanced.keras')\n",
                "print(\"Model saved!\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.11.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}